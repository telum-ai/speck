---
name: ai-api-integration
description: Load when integrating OpenAI and Anthropic Claude APIs for AI-powered features. Applies when implementing structured outputs, streaming responses, function calling, or managing token costs.
---

# AI API Integration (OpenAI & Anthropic Claude)

## When This Rule Applies
- Implementing AI chat or completion features
- Setting up function calling / tool use
- Streaming responses for real-time UX
- Managing costs and token optimization
- Error handling and retry strategies

## Structured Outputs (Schema Enforcement)

**Use structured outputs for reliable JSON responses. Eliminates validation logic!**

### OpenAI (GPT-4o)

```python
from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class ExtractedData(BaseModel):
    name: str
    email: str
    sentiment: str  # "positive" | "negative" | "neutral"

response = client.beta.chat.completions.parse(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Extract: John (john@example.com) loves your product!"}],
    response_format={
        "type": "json_schema",
        "json_schema": {
            "name": "ExtractedData",
            "schema": ExtractedData.model_json_schema(),
            "strict": True
        }
    }
)

data = response.choices[0].message.parsed  # Typed!
```

### Anthropic Claude

```python
from anthropic import Anthropic

client = Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    tools=[{
        "name": "extract_data",
        "description": "Extract structured data",
        "input_schema": {
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "email": {"type": "string"},
                "sentiment": {"type": "string", "enum": ["positive", "negative", "neutral"]}
            },
            "required": ["name", "email", "sentiment"]
        }
    }],
    tool_choice={"type": "tool", "name": "extract_data"},
    messages=[{"role": "user", "content": "Extract: John (john@example.com) loves your product!"}]
)

# Response contains tool_use block with validated data
```

## Streaming Responses

```python
# OpenAI streaming
stream = client.responses.create(
    model="gpt-4o",
    input=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for event in stream:
    if event.type == "response.output_text.delta":
        print(event.delta, end="", flush=True)

# Anthropic streaming
with client.messages.stream(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Tell me a story"}]
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)
```

## Error Handling with Exponential Backoff

```python
import time
from typing import Any

def call_with_retry(fn, max_retries=5, initial_wait=1.0):
    """Retry with exponential backoff and jitter."""
    for attempt in range(max_retries):
        try:
            return fn()
        except (RateLimitError, APIConnectionError) as e:
            if attempt == max_retries - 1:
                raise
            # Exponential backoff with jitter
            wait_time = initial_wait * (2 ** attempt) * (0.5 + random.random())
            time.sleep(min(wait_time, 30))  # Cap at 30s
        except APIStatusError as e:
            if e.status_code >= 500:  # Server errors: retry
                wait_time = initial_wait * (2 ** attempt)
                time.sleep(wait_time)
            else:  # Client errors: don't retry
                raise
    raise Exception("Max retries exceeded")
```

## Cost Optimization

### 1. Prompt Caching (up to 90% savings!)

```python
# Put static content at the beginning
# Variable content at the end
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": LONG_SYSTEM_PROMPT},  # Cached!
        {"role": "user", "content": user_query}  # Variable
    ],
    prompt_cache_key=org_id  # Route to same server
)

# Check cache usage
print(f"Cached tokens: {response.usage.cache_read_tokens}")
```

### 2. Batch API (50% savings)

```python
# For non-time-sensitive workloads
batch_file = client.files.create(
    file=open("batch_requests.jsonl", "rb"),
    purpose="batch"
)

batch = client.batches.create(
    input_file_id=batch_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h"
)
```

### 3. Model Selection

| Task | Recommended Model |
|------|-------------------|
| Simple classification | GPT-4o-mini |
| Code generation | Claude Sonnet 4.5 |
| Complex reasoning | GPT-5 / Claude Opus |
| Embeddings | text-embedding-3-small |

### 4. Token Counting Before Sending

```python
# Anthropic
response = client.messages.count_tokens(
    model="claude-sonnet-4-5",
    messages=[{"role": "user", "content": prompt}]
)
print(f"Input tokens: {response.input_tokens}")
```

## Common Gotchas

### 1. Function Calling is Conversational
```python
# Tool calls require follow-up!
response = client.messages.create(...)  # Returns tool_use

# You MUST send tool result back
if response.stop_reason == "tool_use":
    tool_result = execute_tool(response.content)
    
    follow_up = client.messages.create(
        messages=[
            *original_messages,
            {"role": "assistant", "content": response.content},
            {"role": "user", "content": [{"type": "tool_result", ...}]}
        ]
    )
```

### 2. Streaming Tool Calls Need Accumulation
```python
# Tool parameters arrive as partial JSON
# Accumulate and parse at the end
accumulated_json = ""
for event in stream:
    if event.type == "content_block_delta":
        accumulated_json += event.delta.partial_json

# Only parse when complete
tool_input = json.loads(accumulated_json)
```

### 3. Rate Limits Differ by Metric
- Requests per minute (RPM)
- Tokens per minute (TPM)
- Daily token quota

429 errors don't tell you which limit was hit!

### 4. Context Window Management
```python
# Long conversations overflow context window
# Implement compaction strategy

def compact_history(messages):
    if estimate_tokens(messages) > MAX_CONTEXT * 0.8:
        summary = summarize_messages(messages[:-10])
        return [{"role": "system", "content": summary}] + messages[-10:]
    return messages
```

## Prompt Engineering Tips

### Chain-of-Thought
```
Solve step by step:
1. What am I starting with?
2. What operation do I need?
3. What is the result?
```

### Few-Shot Examples
```
Examples:
Input: "I love this!" → Positive
Input: "It's okay." → Neutral
Input: "Terrible product." → Negative

Now classify: "Best purchase ever!"
```

### Clear Constraints
```
Rules:
- Maximum 50 words
- No technical jargon
- Include a call-to-action
```

## References

- [OpenAI Docs](https://platform.openai.com/docs)
- [Anthropic Docs](https://docs.anthropic.com)
- [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
- [Claude Tool Use](https://docs.anthropic.com/claude/docs/tool-use)

